---
title: "Homework_summary"
author: '19085'
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SC19085}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework 1

## Exercise 3.4

We use the inverse transform method to generate random number from a Rayleigh distribution.
For Rayleigh distribution, when $\mathrm{x} \geq 0, \sigma>0$, its distribution function is:
$$
\mathrm{F}(\mathrm{x})=1-e^{-\frac{x^{2}}{2 \sigma^{2}}}
$$
the inverse function of its ditribution function is:

$$
F^{-1}(\mathrm{x})=\sqrt{-2 \sigma^{2} \ln (1-x)} \quad 0<x<1
$$
So that if $U$ is a randon variable of uniform distribution in $[0,1]$, then random variable $F^{-1}(U)$ obey Rayleigh distribution.
```{r}
# specify sigma from 1 to 5
for(sigma in 1:5){
  # generate 2000 random numbers from uniform distribution
  U<-runif(2000,0,1)
  # generate random numbers from Rayleigh distribution using inverse transform
  rand_num<-sqrt(-2*sigma^2*log(1-U))
  # draw the histogram of the random numbers.
  hist(rand_num,prob=T,breaks=100,main=paste('sigma=',sigma,sep=''))
  # draw the density function of Rayleigh distribution
  x<-seq(0,20,0.01)
  lines(x,x/sigma^2*exp(-x^2/(2*sigma^2)))
}

```

## Exercise 3.11
Firstly, we generate 1000 random numbers from Bernoulli distribution with probability equals to 0.75, then we generate 1000 random numbers from normal distribution $N(0,1)$ and $N(3,1)$ respectively, finally we combine them to generate the mixed distribution.

```{r }
# generate 1000 random numbers from Bernoulli distribution
b<-rbinom(n=1000,size=1,prob = 0.75)
# generate 1000 random numbers from normal distribution N(0,1)
N1<-rnorm(1000,0,1)
# generate 1000 random numbers from normal distribution N(3,1)
N2<-rnorm(1000,3,1)
# combine them to generate the mixed distribution
rand_num_mixed<-b*N1+(1-b)*N2
# draw the histogram and the density line of the random numbers
hist(rand_num_mixed,breaks =100,probability =T)
lines(density(rand_num_mixed))
```


Now, we draw the mixed distribution with probability equals to 0.5.

```{r echo=FALSE}
# generate 1000 random numbers from Bernoulli distribution
b<-rbinom(n=1000,size=1,prob = 0.5)
# generate 1000 random numbers from normal distribution N(0,1)
N1<-rnorm(1000,0,1)
# generate 1000 random numbers from normal distribution N(3,1)
N2<-rnorm(1000,3,1)
# combine them to generate the mixed distribution
rand_num_mixed<-b*N1+(1-b)*N2
# draw the histogram and the density line of the random numbers
hist(rand_num_mixed,breaks =100,probability =T)
lines(density(rand_num_mixed))
```
 
From the pictuer above, we can figure out that When the probality equals to 0.5, the mixed distribution will be a bimodol distribution.

##  Exercise 3.18
We write function **Wishart** to generate a random sample from Wishart distribution, to use Bartlett's decomposition we need to specify three parameters including:

+ Sigma: the covariance matrix.
 
+ n: the number of matrix to be added satisfing $\mathrm{n}>\mathrm{d}+1 \geq 1$.
 
+ d: the dimension of the random vector satisfing $\mathrm{d}\geq 0$.

```{r}
Wishart<-function(Sigma,n,d){
  if (missing(d)) d<-ncol(Sigma) 
   if(missing(n)) n<-d+2
   # generate the matrix with diagnal entries obey the Chi-square distibution 
   # and off-diagnal entries obey normal distribution.
  T<-matrix(0,nrow=d,ncol=d)
  T[1,1]<-sqrt(rchisq(1,df=n))
  for(i in 2:d){
    T[i,i]<-sqrt(rchisq(1,df=n-i+1))
    for(j in 1:i-1){
      T[i,j]<-rnorm(1)
    }
  }
  
  A<-T%*%t(T)
  # calculate the cholesky decomposition of matrix Sigma. 
  L<-chol(Sigma)
  return(t(L)%*%A%*%L)
}
```

# Homework 2

## exercise 5.1

To compute a Monte Carlo estimate $\hat{\theta}$  of $\theta=\int_{0}^{\pi / 3} \sin t d t$, we creat function **Monte_carlo**, the parameter of this function is **n**, which represents the number of replicates. we notice that $\theta=\frac{\pi}{3}*E(\sin X)\ \ X \sim \mathrm{U}[0,\pi/3]$, so that we use sample mean to estimate the mean of the population.

```{r }
Monte_carlo<-function(n){
set.seed(1468)
# generate n random number from distribution U[0,pi/3]
r<-runif(n,min=0,max=pi/3)
# use the sample mean to estimate the population mean
MC_value<-pi/3*mean(sin(r))
# calcullate the true value use function: integrate
true_value<-integrate(sin,0,pi/3)$value
return(c(MC_value,true_value))
}
# simulation 
Monte_carlo(1000000)
```

## exercise 5.10

To use Monte Carlo integration with anithetic variables to estimate  $\theta=\int_{0}^{1} \frac{e^{-x}}{1+x^{2}} d x$, and find the approximate reduction in variance as percentage in contrast with ordinary Monte Carlo method,  we creat function **antithetic_contrast**, the only parameter of this function is **m** which represents the number of replicates.

```{r}
# library(knitr)
antithetic_contrast<-function(m){
  # m should be an even 
  if(m%%2!=0) m<-m+1

  set.seed(1467)
  f<-function(x) return(exp(-x)/(1+x^2))
  r1<-runif(m)
  # calculate the estimate of ordinary Monte Carlo integration and its variance 
  MC_value<-mean(f(r1))
  MC_var<-1/m*var(f(r1))
  
  g<-function(x) return(exp(-x)/(1+x^2)+exp(x-1)/(1+(1-x)^2))
  r2<-runif(m/2)
  # calculate the estimate of Monte Carlo integration with anithetic variables and its variance 
  Anti_value<-1/m*sum(g(r2))
  Anti_var<-1/(2*m)*var(g(r2))
  result<-rbind(c(MC_value,Anti_value),c( MC_var,Anti_var))
  rownames(result)<-c('mean','variance')
  # output the reduction of variance in percentage
  cat('the reduction in variance is',round(100*(MC_var-Anti_var)/ MC_var,2),'%')
  knitr::kable(result, format = 'html', row.names = T, col.names = c('MC','Anti'))
 
} 

antithetic_contrast(1000)

```

## exercise 5.15

We use Stratified Importance sampling to estimate  
$$
\int_{0}^{1} \frac{e^{-x}}{1+x^{2}} d x
$$

firstly, we divide the interval $[0,1]$ into five subintervals by the four fifth-quanliles of the population:
$$
\mathrm{F}(\mathrm{x})=\frac{1-e^{-x}}{1-e^{-1}} \quad 0 \leq x \leq 1
$$
**quant** is the result, with its first number 0 and last number 1, then in each subinterval, the sampling function which also represents the density function of the random variable in each interval changes to 
$$
\frac{5e^{-x}}{1-e^{-1}}
$$

at the same time, the expectation we want to calculate in each subinterval is 
$$
\mathrm{E}\left(\frac{1-e^{-1}}{5\left(1+X^{2}\right)}\right)
$$

to generate random number satisfing our demand, we use acceptance-rejection method, in each subinterval, we calculate the optimal **c**, for **g**, we specified it to be the density function of uniform distridution in each subinterval, and due to the whole number of replicates is 10000, in each subinterval, we generate 2000 random numbers. we repeat the estimate 100 times and use the sample standard deviation to estimate standard deviation of population.


```{r}
F<-function(x) return((1-exp(-x))/(1-exp(-1))) # distribution function
F_inverse<-function(x) return(-log(1-(1-exp(-1))*x)) # the inverse function of distribution function
G<-function(x) return((1-exp(-1))/(5*(1+x^2))) # the function of the random variable, we want to calculate its expectation
f<-function(x) return(5*exp(-x)/(1-exp(-1))) # density function of random variable in each subinterval

quant<-F_inverse(seq(0,1,by=1/5)) # interval endpoints of the 5 subintervals
g<-(quant[-1]-quant[-6])^-1 # the density function of uniform distridution in each subinterval
theta_hat<-numeric(100)
for(k in 1:100){
theta<-numeric(5)
for(i in 1:5){
  # use acceptance-rejection method to generate the random number
 
  random_vector<-numeric(0)
  
  c<-f(quant[i])/g[i]
  while (length(random_vector)<2000) {
    Y<-runif(1,min=quant[i],max=quant[i+1])
    U<-runif(1)
    if(U<(f(Y)/(c*g[i]))) random_vector<-c(random_vector,Y)
  }
  
  theta[i]<-mean(G(random_vector))
  
}


theta_hat[k]<-sum(theta)
}
list(theta_hat=theta_hat,sd=sd(theta_hat))
```

from the result we can figure out that importance sampling with stratified has a prominent standard deviation reduction in contrast with the one without stratified, the original standard deviation is 0.0970314, meanwhile, the estimation becomes more accurate.


# Homework 3

## exercise 6.5

*  empirical confidence level for mean


This question means that we should use a Monte Carlo experiment to estimate the coverage probability of the 95% symmetric  t-interval for random samples of $\chi^{2}(2)$ dataï¼Œto see the probability that the confidence interval covers the mean is higher or lower than 0.95. firstly according to the sampling distribution theorem, if the samle is normal distribution, then:

$$
\frac{\bar{X}-\mu}{S / \sqrt{n}} \sim t(n-1)
$$

where $\mu$ is population mean, for $\chi^{2}(2)$, it equals to 2, $\bar{X}$ is sample mean, $S$ is the sample variance, $n$ is the number of samples. so the 95% symmetric t-interval for population mean $\mu$ is:

$$
\left[\bar{X}-S / \sqrt{n} t_{1-\frac{a}{2}}(n-1), \bar{X}-S / \sqrt{n} t_{\frac{a}{2}}(n-1)\right]
$$

where $\alpha$ is the significance level, we creat function cpt to return the empirical confidence level for mean, the parameter of cpt is :

  * m: the number of random experiment
  * n: the number of random numbers generated in each experiment
  * alpha: the significance level.

```{r}
cpt<-function(m,n=20,alpha=0.05){
 
  # generate matrix of random numbers of dim m*n
  chisq<-matrix(rchisq(m*n,df=2),nrow = m,ncol = n)
  # calculate the symmetric t confidence interval according to each row of random numbers
  interval_cal<-function(x) return(c(mean(x)-var(x)/sqrt(n)*qt(1-alpha/2,df=n-1),
                                 mean(x)-var(x)/sqrt(n)*qt(alpha/2,df=n-1)))
  interval_matix<-t(apply(chisq,1,interval_cal))
  # calculate the empirical confidence level
  return(1/m*sum(2> interval_matix[,1]&2<interval_matix[,2]))
  
}
cpt(m=1000)
```

from the result, the empirical confidence level is close to 0.95, so that the interval estimation for mean is not sensitive to departures from normality.


*  empirical confidence level for variance

in order to compared with example 6.4, we conduct 1000 random experiments too, and calculate empirical confidence intervals for variance:
$$
\left(0,(n-1) S^{2} / \chi_{\alpha}^{2}\right)
$$
for  $\chi^{2}(2)$, the variance is 4.

```{r}
upper_bound<-replicate(1000,expr={
  n<-20
  alpha<-0.05
  x <- rchisq(n, df=2)
  (n-1) * var(x) / qchisq(alpha, df = n-1)
})
cpt_variance<-mean(upper_bound>4)

cat('the empirical confidence level for variance is:', cpt_variance)
```
from the result, we know that if the population don't obey normal istribution, the empirical confidence level for variance is far lower than 0.95, so that The t-interval is more robust to departures from normality than the interval for variance.



## exercise 6.6

to estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness under normality by a Monte Carlo experiment, we creat function skew_quant, we use formula 2.14:

$$
\operatorname{Var}\left(\hat{x}_{q}\right)=\frac{q(1-q)}{n f\left(x_{q}\right)^{2}}
$$
to calculate variance of the q sample quantile, where $f$ is the pdf of $N(0,6 / n)$, $x_{q}$ is the true q-quantile of skewness, the parameter of skew_quant is:

  * m: the number of random experiments
  * n: the number of random numbers generated in each experiment 
  
```{r}
skew_quant<-function(m,n){
  
  # f is the pdf of N(0,6/n)
  f<-function(x) return(sqrt(n/(12*pi))*exp(-n*x^2/12))
  quant<-c(0.025,0.05,0.95,0.975)
  # generate matrix of random numbers obey normal distribution 
  norm_matrix<-matrix(rnorm(m*n),nrow = m,ncol = n)
  skew<-function(x) return(mean((x-mean(x))^3/sd(x)^3))
  skew_hat<-t(apply(norm_matrix,1,skew))
  # calculate sample quantiles
  skew_quant<-quantile(skew_hat,prob=quant)
  # true quantiles generate form normal distribution N(0,6/n)
  true_quant<-qnorm(quant,mean=0,sd=sqrt(6/n))
  # calculate the standard error of sample quantiles
  skew_sd<-sqrt(quant*(1-quant)/(n*f(true_quant)^2))

  return(rbind(skew_quant,true_quant,skew_sd))
}

skew_quant(2000,500)
```

from the above result, the estimated quantiles by Monte Carlo is close to real quantiles of the large sample approximation,and the standard deviation is both low for 0.025,0.05,0.95,0.975 sample quantiles.


# Homework 4


## 6.7

**question** :estimate the power of the skewness test of normality against symmetric $\beta(\alpha,\alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(Î½)$?

```{r }
library(knitr)
sig <-0.05 # significance level
m <-1000 # times of simulations
n <-500 # number of replications in each simulation
cv <- qnorm(1-sig/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))

# sk is the fuction used to compute the sample skewness coeff.

sk <- function(x) {
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}

alpha<- seq(0.5,10,by=0.5) # parameter of symmetric beta distribution
power<-numeric(length(alpha))

for(i in 1:length(alpha)){
   reject_i<-replicate(m,expr={
   rand_num<-rbeta(n,alpha[i],alpha[i])  
   skew<-sk(rand_num) 
   as.integer(abs(skew)>=cv) 
     
   })
   
  power[i]<-mean(reject_i)
}
plot(alpha ,power,type='l', xlab='alpha',ylab='power', main='power versus alpha for Beta(alpha,alpha)')
knitr::kable (rbind(alpha,power),format = 'html',row.names = T,digits = 2)
  
```

from the result above, we can know that the power is lower than 0.05, and it has an increasing trend as alpha increase but not strictly, this is because skewness test mainly test symmetrical features of a distribution and beta distribution has this features.

```{r}

# the degree of freedom of the T distribution
v<-1:100

m <-1000 # times of simulations
n <-500 # number of replications in each simulation
cv <- qnorm(1-sig/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3)))) # the critical value

# store the power 
power_t<-numeric(100)

for(j in v){

reject.t<- vector('numeric',length=m)

for(i in 1:m){
  rand_num<-rt(n,df=j)
  skew<-sk(rand_num)
  reject.t[i]<-as.integer(abs(skew)>=cv)
}
power_t[j]<-mean(reject.t)
}

plot(v,power_t,xlab='df',ylab='power',type='l',main=' power versus v for t(v)')

abline(h=0.1)
```

from the power curve for t distribution, we can conclude that the power is very high when df is small, and it is decreasing as df increasing, this is because when the df is large, the limit distribution of t distribution is normal distribution, so that the power will be low if df is very large.

## 6A

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^{2}(1)$, (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test H0 : $Î¼ = Î¼0$ vs H1 : $Î¼ \ne Î¼0$, where $Î¼0$ is the mean of $\chi^{2}(1)$, Uniform(0,2), and Exponential(1), respectively.

```{r }
set.seed(548546)

alpha<-0.05 # significance level


n<-100 # number of replicates in each simulation
m<-1000 # number of simulations



##  the codes below is to calculate empirical Type I error rate for normal distribution

# reject.norm<-vector('numeric',m)
# for(i in 1:m){
 # rand_num<-rnorm(n,mean=1)
  
  #pvalue<-t.test(rand_num, alternative = 'two.sided',mu=1, conf.level = 1-alpha)$p.value
  #reject.norm[i]<-ifelse(pvalue<alpha,1,0)
 # }

#t1e.norm<-mean(reject.norm)
 
## 


# calculate empirical Type I error rate for chisq distribution
reject.chisq<-vector('numeric',m)
 for(i in 1:m){
  rand_num<-rchisq(n,df=1)
  pvalue<-t.test(rand_num, alternative = 'two.sided',mu=1, conf.level = 1-alpha)$p.value
  reject.chisq[i]<-ifelse(pvalue<alpha,1,0)
 } 

t1e.chisq<-mean(reject.chisq)


# empirical Type I error rate for uniform distribution
reject.unif<-vector('numeric',m)
 for(i in 1:m){
  rand_num<-runif(n,min=0,max=2)
  pvalue<-t.test(rand_num, alternative = 'two.sided',mu=1, conf.level = 1-alpha)$p.value
  reject.unif[i]<-ifelse(pvalue<alpha,1,0)
 } 

t1e.unif<-mean(reject.unif)
               

# calculate empirical Type I error rate for exponential distribution 
reject.exp<-vector('numeric',m)
 for(i in 1:m){
  rand_num<-rexp(n,rate=1)
  pvalue<-t.test(rand_num, alternative = 'two.sided',mu=1, conf.level = 1-alpha)$p.value
  reject.exp[i]<-ifelse(pvalue<alpha,1,0)
 } 

t1e.exp<-mean(reject.exp)

t1e<-c(t1e.chisq,t1e.unif,t1e.exp)
names(t1e)<-c('Ï‡2(1)','U[0,2]','E(1)')
print(t1e)
  

```

from the result above, we know that the empirical Type I error rate for $\chi^{2}(1)$, $U[0,2]$, $E(1)$ distribution is 0.060,0.055,0.053, so for $U[0,2]$ and $E(1)$, the empirical Type I error rate of the t-test is close to the nominal significance level $\alpha =0.05$, and for $\chi^{2}(1)$, the empirical Type I error rate is larger than 0.05.

# Homework 5

## 7.6

* question:
Use a panel display to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of the standard errors for each of the following estimates: $\hat{\rho}_{12}=\hat{\rho}(\mathrm{mec}, \mathrm{vec})$,$\hat{\rho}_{34}=\hat{\rho}(\mathrm{alg}, \mathrm{ana})$,$\hat{\rho}_{35}=\hat{\rho}(\mathrm{alg}, \mathrm{sta})$,$\hat{\rho}_{45}=\hat{\rho}(\mathrm{ana}, \mathrm{sta})$.

```{r}
library(boot)
library(bootstrap)
attach(scor)
set.seed(1468)
# draw the scatterplot of each pair of test scores
pairs(scor,main='the scatter plots for each pair of test scores')

# calculate the sample correlation matrix
cor(scor)

# calculate the bootstrap estimates of the standard error

stat<-function(data,ind){
  
  cor12<-cor(data[ind,1],data[ind,2])
  cor34<-cor(data[ind,3],data[ind,4])
  cor35<-cor(data[ind,3],data[ind,5])
  cor45<-cor(data[ind,4],data[ind,5])
  
  
  return(c(cor12,cor34,cor35,cor45))
}

Boot<-boot(scor,stat,R=2000)

sd.boot<-apply(Boot$t,2,sd)

sd.boot
```

from the scatterplot of each pair of test scores, we can know that the scores of each subject are positively correlated, the most relevant scores are algebra and analysis, the correspond trend of positive correlation in scatterplot is obvious and steep, the least relevent scores are mechanics and stastics, the correspond trend of positive correlation in scatterplot is not obvious, and the standard error of correalation coefficients are 0.0771,0.0476,0.0606,0.0667.


## project 7.B

* question:Compare the coverage rates for normal populations (skewness 0) and $\chi^{2}(5)$ distributions (positive skewness).we can calculate that the skewness of $\chi^{2}(5)$ is $\sqrt\frac{8}{5}$.

```{r }
library(boot)
set.seed(1468)
sk <- function(x,ind) {
xbar <- mean(x[ind])
m3 <- mean((x[ind] - xbar)^3)
m2 <- mean((x[ind] - xbar)^2)
return( m3 / m2^1.5 )
}
m<-500
n<-200
ci.n.norm<-ci.n.basic<-ci.n.perc<-matrix(0,m,2)
for(i in 1:m){
  data<-rnorm(n)
  boot.skew<-boot(data,statistic=sk,R=500)
  ci.n<-boot.ci(boot.skew,type=c('norm','basic','perc'))
  ci.n.norm[i,]<- ci.n$norm[2:3]
  ci.n.basic[i,]<- ci.n$basic[4:5]
  ci.n.perc[i,]<- ci.n$percent[4:5]
}
 
ci.chisq.norm<-ci.chisq.basic<-ci.chisq.perc<-matrix(0,m,2)

for(i in 1:m){
  data<-rchisq(n,df=5)
  boot.skew<-boot(data,statistic=sk,R=500)
  ci.chisq<-boot.ci(boot.skew,type=c('norm','basic','perc'))
  ci.chisq.norm[i,]<- ci.chisq$norm[2:3]
  ci.chisq.basic[i,]<- ci.chisq$basic[4:5]
  ci.chisq.perc[i,]<- ci.chisq$percent[4:5]
}

cat('for normal distribution the coverage rate:\n',
    'norm=',mean(ci.n.norm[,1]<=0 & ci.n.norm[,2]>=0),
    'basic=',mean(ci.n.basic[,1]<=0 & ci.n.basic[,2]>=0),
    'perc=',mean(ci.n.perc[,1]<=0 & ci.n.perc[,2]>=0)
    )


cat('for normal distribution missing left:\n',
    'norm=',mean(ci.n.norm[,1]>0 ),
    'basic=',mean(ci.n.basic[,1]>0),
    'perc=',mean(ci.n.perc[,1]>0)
    )

cat('for normal distribution missing right:\n',
    'norm=',mean( ci.n.norm[,2]<0),
    'basic=',mean(ci.n.basic[,2]<0),
    'perc=',mean(ci.n.perc[,2]<0)
    )

cat('for Chi-square distribution the coverage rate:\n',
    'norm=',mean(ci.chisq.norm[,1]<=sqrt(8/5) & ci.chisq.norm[,2]>=sqrt(8/5)),
    'basic=',mean(ci.chisq.basic[,1]<=sqrt(8/5) & ci.chisq.basic[,2]>=sqrt(8/5)),
    'perc=',mean(ci.chisq.perc[,1]<=sqrt(8/5) & ci.chisq.perc[,2]>=sqrt(8/5))
    )

cat('for Chi-square distribution missing left:\n',
    'norm=',mean(ci.chisq.norm[,1]>sqrt(8/5) ),
    'basic=',mean(ci.chisq.basic[,1]>sqrt(8/5)),
    'perc=',mean(ci.chisq.perc[,1]>sqrt(8/5))
    )

cat('for Chi-square distribution missing right:\n',
    'norm=',mean(ci.chisq.norm[,2]<sqrt(8/5)),
    'basic=',mean(ci.chisq.basic[,2]<sqrt(8/5)),
    'perc=',mean( ci.chisq.perc[,2]<sqrt(8/5))
    )


```


# Homework 6

## 7.8

*  question: Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{theta}$.

*  solution: we calcualte the jackknife estimates of bias and standard error of $\hat{theta}$ following the standard steps, which means:

$$bisa_{jack}=(n-1)(\bar{\hat\theta}_{(\cdot)}-\hat\theta)$$

$$ sd_{jack}(\hat\theta) = \sqrt{\frac{n-1}n\sum_{i=1}^n(\hat\theta_{(i)}-\bar{\hat\theta}_{(\cdot)})^2}$$

```{r }
library(bootstrap)
attach(scor)
len<-nrow(scor)

# theta.dot.hat stores the estimators without one observation
theta.dot.hat<-vector(length = len)

C<-cov(scor)
eigen.value<-eigen(C,only.values = T)$values

# theta.hat is the estimator with all observations
theta.hat<-max(eigen.value)/sum(eigen.value)

for(i in 1:len){
  C<-cov(scor[-i,])
  eigen.value<-eigen(C,only.values = T)$values
  theta.dot.hat[i]<-max(eigen.value)/sum(eigen.value)
}

# calculate the bias
bias<-(len-1)*(mean(theta.dot.hat)-theta.hat)

# calculate the sd
standard_dev<-sqrt((len-1)*mean((theta.dot.hat-mean(theta.dot.hat))^2))

print(c(bias,standard_dev))
```

## 7.10

*  question: In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$?

```{r }

library(DAAG); attach(ironslag)

n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
y <- magnetic[-k]
x <- chemical[-k]

# linear model
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[k] <- magnetic[k] - yhat1

summary(J1)
# quadratic

J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] - yhat2
summary(J2)

# exponential
J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3
summary(J3)

# cubic polinomial
J4 <- lm(y~x+I(x^2)+I(x^3))
yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k]
          +J4$coef[3] * chemical[k]^2 + J4$coef[4] * chemical[k]^3
e4[k] <- magnetic[k] - yhat4
}

e<-c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
names(e)<-c('linear model','quadratic','exponential','cubic polinomial')
print(e)


```

from the above result we know that quadratic model is selected by the cross validation procedure, however when we use function **sumary** we can get the adjusted $R^2$ for each model:

```{r}
summary(J4)
```

the adjusted $R^2$ for linear model, quadratic model, exponential model, cubic model is 0.5319, 0.591, 0.5283, 0.5937, the maximum one is cubic polynomial model, so that the cubic polynomial model is supposed to be chosen by adjusted $R^2$.

# Homework 7

## 8.3

* question: The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

* solution: we use function **maxout** to calculate maximum number of extreme points in each (x,y) pair, if the null hypothesis is right, which means the variances of the two populations are same, then, after permuting z = c(x,y), we designate first n1 elements to be x, the left n2 elements to be y, $ n1 \neq n2$, we get a replicate, their variances also should be equal, we calculate maximum number of extreme points using this replicate. Repeating this procedure many times, we can calculate p value, meanwhile, by repeating the experiments many times, we can calculating type I error rate and the power.

```{r }
library(boot)
set.seed(1468)

# calculate maximum number of extreme points for pair x,y
maxout <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(max(c(outx, outy)))
}

# the statistics passed to boot
stat<-function(z,ix,n){
  x<-z[ix][1:n]
  y<-z[ix][-(1:n)]
  maxout(x,y)
}

# this function is used to calculate p value
permu_count5<-function(n1,n2,mu=0,sd1,sd2){
  x<-rnorm(n1,mu,sd1)
  y<-rnorm(n2,mu,sd2)
  z<-c(x,y)
  R=999
  boot_obj<-boot(z,statistic = stat,R=R,sim='permutation',n=n1)
  count<-c( boot_obj$t0, boot_obj$t)
  
  p.value<-mean(count>=count[1])
  return(p.value)
}
n<-1000
p_value<-numeric(n)

# calculate the empirical type I error rate
for(i in 1:n) p_value[i]<-permu_count5(n1=20,n2=30,sd1=1,sd2=1)
cat('the empirical type I error rate is:',mean(p_value<0.05),'\n')
# calculate the power
for(i in 1:n) p_value[i]<-permu_count5(n1=20,n2=30,sd1=1,sd2=2)
cat('the empirical power is:',mean(p_value<0.05))
```

## Test for independence

* solution: we need to test the independence between X, Y1 for model1 and X, Y2 for model2, to solve this problem, we use function **dcov.test** in package **energy** to make a distance correlation test and we use function **bcov.test** in package **ball** to make a ball test, for convinience, we denote the power of **dcov.test** for model1 and model2 by power.dcov1, power.dcov2 respectively, similarly we denote the power of **bcov.test** by power.ball1, power.ball2, We calculate the power of two tests with respect to number of samples for model1 and model2 and draw the the power function image. 

```{r message=FALSE}
library(energy)
library(Ball)
library(mixtools)

# n is the sample size
# m is the number of experiments of each sample size n
# R is the number of permutation replicates for calculating p value
power.contrast<-function(m,n,R){
  
  pvalue.ball1<-pvalue.dcov1<-numeric(m)
  pvalue.ball2<-pvalue.dcov2<-numeric(m)
 
  for(i in 1:m){
  
  # generate samples
  X<-rmvnorm(n=n,mu=c(0,0),sigma=diag(c(1,1)))
  e<-rmvnorm(n=n,mu=c(0,0),sigma=diag(c(1,1)))
  Y1<-X/4+e 
  Y2<-X/4*e 
  
     seed<-runif(1)
     set.seed(seed)
     pvalue.dcov1[i]<-dcov.test(x=X,y=Y1,R=R)$p.value
     pvalue.ball1[i]<-bcov.test(x=X,y=Y1,R=R,seed=seed)$p.value
     pvalue.dcov2[i]<-dcov.test(x=X,y=Y2,R=R)$p.value
     pvalue.ball2[i]<-bcov.test(x=X,y=Y2,R=R,seed=seed)$p.value
  }
  power.dcov1<-mean(pvalue.dcov1<0.01)
  power.ball1<-mean(pvalue.ball1<0.01)
  power.dcov2<-mean(pvalue.dcov2<0.01)
  power.ball2<-mean(pvalue.ball2<0.01)
  
  return(c(power.dcov1,power.ball1,power.dcov2,power.ball2))
  
}


n<-seq(50,200,by=10)
power<-matrix(0,nrow=length(n),ncol=4)
for(i in 1:length(n)) power[i,]<-power.contrast(m=200,n=n[i],R=200)
plot(n,power[,1],type='b',lty=1,pch=0,main='power for model1',xlab='n',ylab='power')
lines(n,power[,2],lty=2)
points(n,power[,2],pch=6)
legend("topleft",legend=c('dcov','ball'),pch=c(0,6),lty = c(1,2))


plot(n,power[,3],type='b',lty=1,pch=0,main='power for model2',xlab='n',ylab='power')
lines(n,power[,4],lty=2)
points(n,power[,4],pch=6)
legend(x=180,y=0.6,legend=c('dcov','ball'),pch=c(0,6),lty = c(1,2))

```

from the result, we can conclude that under the significance level $\alpha=0.01$, the power of distance correlation test is better than ball test for both models, when the sample size n is large enough, the power of distance correlation test can reach 1, on the contrary, the power of ball test keeps 0 no matter how sample size changed.

# Homework 8

## exercise 9.4

*  question: Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of
each chain.


*  solution: For standard Laplace distribution, the pdf is:
$$
f(x)=\frac{1}{2}e^{-|x|}
$$

its mean is 0 and its standard deviation is $\sqrt{2}$, so most of the random numbers should within the range $\lbrack -3\sqrt{2},3\sqrt{2}\rbrack$.

For the increment, we choose normal distibution with mean equals 0 and sd equals 0.05,0.5,2,16, we use function **rw.Metropolis** to plot the chains generated by different increments and calculate the acceptance rates.
```{r }
set.seed(1468)
# pdf of standard Laplace distribution
laplace<-function(x) return(1/2*exp(-abs(x)))

rw.Metropolis <- function(sigma, x0, N) {
  
# N is the number of iterations
x <- numeric(N)
# x0 is the initial value
x[1] <- x0

# u determines whether accept Y as x(t+1) or not
u <- runif(N)

# k denotes the times of rejection
k <- 0

for (i in 2:N) {
# the candidate is from x[i-1] plus a normal increment ~ N(0,sigma)
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (laplace(y) / laplace(x[i-1])))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}


sigma <- c(.05, .5, 2, 16)
N=2000
x0 <- 25
rw1 <- rw.Metropolis( sigma[1], x0, N)
rw2 <- rw.Metropolis( sigma[2], x0, N)
rw3 <- rw.Metropolis( sigma[3], x0, N)
rw4 <- rw.Metropolis( sigma[4], x0, N)


##par(mfrow=c(2,2))
plot(1:2000,rw1$x,type='l',ylab="x",xlab='iteration',main='sd=0.05')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,rw2$x,type='l',ylab="x",xlab='iteration',main='sd=0.5')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,rw3$x,type='l',ylab="x",xlab='iteration',main='sd=2')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,rw4$x,type='l',ylab="x",xlab='iteration',main='sd=16')
abline(h=c(-3*sqrt(2),3*sqrt(2)))

accept_rate<-c(1-rw1$k/(N-1), 1-rw2$k/(N-1), 1-rw3$k/(N-1), 1-rw4$k/(N-1))
names(accept_rate)<-c('sd=0.05','sd=0.5','sd=2','sd=16')
print(accept_rate)


```

From the result of the acceptance rates, we can conclude that, none of the four acceptance rates is within the range$\lbrack 0.15,0.5\rbrack$, however, the third one is the nearest among the four increments, when sd equals 16, the acceptance rate is very low, so that we reject most of the candidate random numbers, this condition is inefficient. On the contrary, when sd equals 0.05, most of the candidates points are accepted so that the convergence speed is very slow.

In conclusion, when sd equals 0.5 or 2, the random walk Metropolis sampler performs well.


# Homework 9


## 11.1

*  Question: The natural logarithm and exponential functions are inverses of each other, so that mathematically log(exp x) = exp(logx) = x. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality?

*  Solution:

```{r cars}
# example
x<-100
x1<-log(exp(x))
x2<-exp(log(x))
x1==x2
all.equal(x1,x2)

x<-0.001
x1<-log(exp(x))
x2<-exp(log(x))
x1==x2
all.equal(x1,x2)


```



From the result, we can know that the property does not hold exactly when s is extremely large or extremely small, and the identity hold with near equality.


## 11.5


*  Question: Write a function to solve the equation:

$$
\begin{aligned} \frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)} \Gamma\left(\frac{k-1}{2}\right)} \int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k / 2} d u & \\
 = \frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1) / 2} d u \end{aligned}
$$

for $a$, where:

$$
c_{k}=\sqrt{\frac{a^{2} k}{k+1-a^{2}}}
$$

Compare the solutions with the points $A(k)$ in Exercise 11.4.

*  Solution: we write function **f_1**, **f_2** to denote the equation to be solved and use function **uniroot** to find the solution in the interval $(0,\sqrt{k})$

```{r}
library(knitr)

# 11.5
CK<-function(a,k) return(sqrt(a^2*k/(k+1-a^2)))

Coef1<-function(k) return(sqrt(4/(pi*(k-1)))*exp(lgamma(k/2) - lgamma((k - 1)/2)))

Coef2<-function(k) return(sqrt(4/(pi*k))*exp(lgamma((k+1)/2) - lgamma(k/2)))

f1<-function(u,k) return((1+u^2/(k-1))^(-k/2))
f2<-function(u,k) return((1+u^2/k)^(-(1+k)/2))

# f_1=0 is the equation to be solved
f_1<-function(a,k){
  ck<-CK(a=a,k=k)
  ck_1<-CK(a=a,k=k-1)
  coef1<-Coef1(k=k)
  coef2<-Coef2(k=k)
  integ1<-integrate(f1, lower=0, upper=ck_1,
    rel.tol=.Machine$double.eps^0.25,
    k=k)$value
  
  integ2<-integrate(f2, lower=0, upper=ck,
    rel.tol=.Machine$double.eps^0.25,
    k=k)$value
  
 coef1*integ1-coef2*integ2
}

# we use function solu_find to find the solution
solu_find<-function(k){
 interval<-c(0.01,sqrt(k)-0.05)
 uniroot(f_1,interval = interval, k=k)$root
}



# 11.4

## f_2 and intersec are same as 11.5
f_2<-function(a,k){
  bound1<-1-pt(sqrt(a^2*(k-1)/(k-a^2)),df=k-1)
  bound2<-1-pt(sqrt(a^2*k/(k+1-a^2)),df=k)
  bound1-bound2
}
intersec<-function(k){
uniroot(f_2,interval = c(0.01,sqrt(k)-0.01),k=k)$root}


k<-c(4:25,100,500,1000)
root<-matrix(0,nrow=length(k),ncol=2)
colnames(root)<-c('a1','a2')
rownames(root)<-as.character(k)
for(i in 1:length(k)){
  root[i,1]<-intersec(k[i])
 # root[i,2]<-solu_find(k[i])
}

for(i in 1:16){
 root[i,2]<-solu_find(k[i])
}

root[17,2]<-uniroot(f_1,interval = c(0.01,4), k=20)$root
root[18,2]<-uniroot(f_1,interval = c(0.01,4), k=21)$root
root[19,2]<-uniroot(f_1,interval = c(0.01,4), k=22)$root

root[20,2]<-uniroot(f_1,interval = c(1,sqrt(23)-0.01), k=23)$root
root[21,2]<-uniroot(f_1,interval = c(0.01,sqrt(24)-0.01), k=24)$root
root[22,2]<-uniroot(f_1,interval = c(0.01,5-0.01), k=25)$root
root[23,2]<-uniroot(f_1,interval = c(0.01,5), k=100)$root
root[24,2]<-uniroot(f_1,interval = c(0.01,10*sqrt(5)-0.1), k=500)$root
root[25,2]<-uniroot(f_1,interval = c(0.01,2), k=1000)$root
knitr::kable(root,format = 'html')
```


From the result above, we know that, when $k\leq 22$, the solutions between these two questions are similiar, however, when $k>22$, the solution for 11.4 grows much faster than the solution for 11.5 as k increases.


## Slides:

*   Question:


**  Use EM algorithm to solve MLE of p and q (consider missing
data nAA and nBB).


**  Show that the log-maximum likelihood values in M-steps are
increasing via line plot.

*  Solution:

```{r}
library(rootSolve)
nA<-28
nB<-24
nOO<-41
nAB<-70
MLE<-function(p0,q0){
  C1<-2*nA*p0/(2-p0-2*q0)
  C2<-2*nA*(1-p0-q0)/(2-p0-2*q0)
  C3<-2*nB*(1-p0-q0)/(2-q0-2*p0)
  D1<-2*nB*q0/(2-q0-2*p0)
  
  model<-function(x){
    f1<-(-C1-2*nOO-nAB-2*C2-C3)*x[1]+(-C1-nAB-C2)*x[2]+C1+C2+nAB
    f2<-(-D1-nAB-C3)*x[1]+(-D1-2*nOO-nAB-2*C3-C2)*x[2]+D1+nAB+C3
    c(F1=f1,F2=f2)
  }
  
  return(multiroot(f=model,start=c(p0,q0))$root)
}

# set the maximum number of iterations to 50
N<-50
PQ<-matrix(0,nrow = N,ncol=2)
colnames(PQ)<-c('p','q')

# the initial value of p and q is 1/2,1/3
p0<-1/2
q0<-1/3
tol <- .Machine$double.eps^0.5

for (i in 1:N) {
old<-c(p0,q0)
new<-MLE(p0,q0)
if (sum(abs((new - old)/old)) < tol) {
  iter<-i-1
  break
}
PQ[i,]<-new
p0<-new[1]
q0<-new[2]
}

cat('EM algorithm converged in',iter,'iterations',sep=' ')

# calculate the log-maximum likelihood values in M-steps 
PQ<-rbind(c(1/2,1/3),PQ[1:10,])
loglik<-function(p0,q0,p,q){
  return(2*nA*p0/(2-p0-2*q0)*log(p)+2*nB*q0/(2-q0-2*p0)*log(q)+
           2*nOO*log(1-p-q)+nAB*log(2*p*q)+2*nB*(1-p0-q0)/(2-q0-2*p0)*
           log(2*q*(1-p-q))+2*nA*(1-p0-q0)/(2-p0-2*q0)*log(2*p*(1-p-q)))
}
max_log_lik<-numeric(10)
for(i in 1:10) max_log_lik[i]<-loglik(PQ[i,1],PQ[i,2],PQ[i+1,1],PQ[i+1,2])
plot(1:10,max_log_lik,xlab='iter',ylab='loglik',type='b',pch=19,main='the log-maximum likelihood values in M-steps')
```

From the result, we can conclude that the MLE of p and q calculated by EM algorithm is `r PQ[11,1]` and `r PQ[11,2]`, from the figure above, the log-maximum likelihood values in M-steps are increasing and converge in 10 iterations.

# Homework 10

## 3

*  question: Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in list formulas.

*  solution:
```{r }

formulas<-list(
  mpg~disp,
  mpg~I(1/disp),
  mpg~disp+wt,
  mpg~I(1/disp)+wt
)

# lapply
fit1<-lapply(1:4,function(i) lm(formula = formulas[[i]],data=mtcars))

# forloop

fit2<-vector('list',length = 4)
for(i in seq_along(formulas)) fit2[[i]]<-lm(formula = formulas[[i]],data=mtcars)
  
```



## 4 

*  question: Fit the model mpg ~ disp to each of the bootstrap replicates
of mtcars in the list below by using a for loop and lapply().
Can you do it without an anonymous function?

*  solution:

```{r}
bootstrap<-lapply(1:10,function(i){
  rows<- sample(1:nrow(mtcars),rep=T)
  mtcars[rows,]
})

# lapply without anonymous function

fit.1<-lapply(bootstrap, FUN = lm, formula=mpg~disp)

# for loop

fit.2<-vector('list',length = 10)
for(i in seq_along(bootstrap)) fit.2[[i]]<-lm(mpg~disp,data=bootstrap[[i]])

```


## 5

*  question: For each model in the previous two exercises, extract R2 using
the function below.

*  solution:

```{r mtcars}
rsq<-function(mod) summary(mod)$r.squared

# exercise 3
R2.1<-sapply(fit1,rsq)
R2.2<-sapply(fit2,rsq)
rbind(R2.1,R2.1)

# exercise 4
R2_1<-sapply(fit.1,rsq)
R2_2<-sapply(fit.2,rsq)
rbind(R2_1,R2_2)
```

## 3

*  question: The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.

*  solution:

```{r }
trails<-replicate(
  100,
  t.test(rpois(10,10),rpois(7,10)),
  simplify=FALSE
)

# sapply with anonymous function
pvalue<-sapply(trails,function(test) test$p.value)

# sapply without anonymous function
pvalue1<-sapply(trails,'[[','p.value')

pvalue
pvalue1
```


## 7

*  question: Implement mcsapply(), a multicore version of sapply(). Can
you implement mcvapply(), a parallel version of vapply()?
Why or why not?

*  solution: I think **mvapply** can be implemented, since for vapply, we can also compute each element in any order, itâ€™s natural to dispatch the tasks to different cores, and compute them in parallel. 

```{r}
library(parallel)
cl<-makeCluster(getOption("cl.cores", 2))

# use parallel
#mcsapply<-function(X,FUN,mc.cores=1L){
  
 # res<-mclapply(X,FUN = FUN, mc.cores = mc.cores)
 #simplify2array(res)
#}


# use parLapply
mcsapply2<-function(cl = cl, X, FUN, simplify = TRUE,USE.NAMES = TRUE, chunk.size = NULL){
  parSapply(cl = cl, X=X, FUN=FUN, simplify = simplify,
          USE.NAMES = USE.NAMES, chunk.size = chunk.size)
}

# time spent by sapply
# (sapply(trails,'[[','p.value'))

# time spent by mcsapply
# system.time(mcsapply2(cl,trails,function(test) test$p.value))

# parSapply(cl,trails , function(test) test$p.value)
```


# Homework 11

## Question 1

 Rewrite an Rcpp function for the same task as in exercise 9.4.

## Solution 1

We write function **rw.metropolis** as the  R random number generater, and **Metropolis** as the C++ random number generater (see attached), then we draw the figures under different variance scenarios.

```{r}
library(Rcpp)
## 1.  R random number generater

# pdf of standard Laplace distribution
laplace<-function(x) return(1/2*exp(-abs(x)))

rw.Metropolis <- function(sigma, x0, N) {
  
# N is the number of iterations
x <- numeric(N)
# x0 is the initial value
x[1] <- x0

# u determines whether accept Y as x(t+1) or not
u <- runif(N)

# k denotes the times of rejection
k <- 0

for (i in 2:N) {
# the candidate is from x[i-1] plus a normal increment ~ N(0,sigma)
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (laplace(y) / laplace(x[i-1])))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}


## 2. C++ random number generater: function(Metropolis)
 sourceCpp('~/Metropolis.cpp')


sigma <- c(.05, .5, 2, 16)
N=2000
x0 <- 25

rw1 <- rw.Metropolis( sigma[1], x0, N)
rw2 <- rw.Metropolis( sigma[2], x0, N)
rw3 <- rw.Metropolis( sigma[3], x0, N)
rw4 <- rw.Metropolis( sigma[4], x0, N)

cpp.rw1<-Metropolis( sigma[1], x0, N)
cpp.rw2<-Metropolis( sigma[2], x0, N)
cpp.rw3<-Metropolis( sigma[3], x0, N)
cpp.rw4<-Metropolis( sigma[4], x0, N)

#par(mfrow=c(2,2))
plot(1:2000,rw1$x,type='l',ylab="x",xlab='iteration',main='sd=0.05(R)')

plot(1:2000,rw2$x,type='l',ylab="x",xlab='iteration',main='sd=0.5(R)')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,rw3$x,type='l',ylab="x",xlab='iteration',main='sd=2(R)')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,rw4$x,type='l',ylab="x",xlab='iteration',main='sd=16(R)')
abline(h=c(-3*sqrt(2),3*sqrt(2)))

#par(mfrow=c(2,2))
plot(1:2000,cpp.rw1$x,type='l',ylab="x",xlab='iteration',main='sd=0.05(Cpp)')
plot(1:2000,cpp.rw2$x,type='l',ylab="x",xlab='iteration',main='sd=0.5(Cpp)')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,cpp.rw3$x,type='l',ylab="x",xlab='iteration',main='sd=2(Cpp)')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
plot(1:2000,cpp.rw4$x,type='l',ylab="x",xlab='iteration',main='sd=16(Cpp)')
abline(h=c(-3*sqrt(2),3*sqrt(2)))
```


# Question 2

Compare the generated random numbers by the two functions using qqplot.

# Solution 2

Because after 500 iterations, the distribution of the random numbers generated by both methods are stable except the scenario sd=0.05, so we use **qqplot** to contrast the data points after the 500th iterations. 

```{r}
#par(mfrow=c(2,2))
qqplot(rw1$x[500:2000],cpp.rw1$x[500:2000],xlab='R',ylab='cpp',main='sd=0.05')
qqplot(rw2$x[500:2000],cpp.rw2$x[500:2000],xlab='R',ylab='cpp',main='sd=0.5')
qqplot(rw3$x[500:2000],cpp.rw3$x[500:2000],xlab='R',ylab='cpp',main='sd=2')
qqplot(rw4$x[500:2000],cpp.rw4$x[500:2000],xlab='R',ylab='cpp',main='sd=16')
```

As the qqplots show, the distributions of the random numbers are similiar when sd = 0.5 or 2, when sd = 0.05, since both distributions are not stable so that they are not similiar, when sd = 16, Although most of the candidates are rejected, however, they are also similar in the interval [-2,2].


# Question3
Campare the computation time of the two functions with microbenchmark.

# Solution 3
```{r}
library(microbenchmark)
ts1<-microbenchmark(R=rw.Metropolis(0.05,25,2000),cpp=Metropolis(0.05,25,2000))

ts2<-microbenchmark(R=rw.Metropolis(0.5,25,2000),cpp=Metropolis(0.5,25,2000))

ts3<-microbenchmark(R=rw.Metropolis(2,25,2000),cpp=Metropolis(2,25,2000))

ts4<-microbenchmark(R=rw.Metropolis(16,25,2000),cpp=Metropolis(16,25,2000))

summary(ts1)[,c(1,3,5,6)]
summary(ts2)[,c(1,3,5,6)]
summary(ts3)[,c(1,3,5,6)]
summary(ts4)[,c(1,3,5,6)]
```

